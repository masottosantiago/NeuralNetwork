{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Functions\n",
    "\n",
    "def Get_Files (setfile,labelfile):\n",
    "     \n",
    "    with open(setfile, 'r') as file:\n",
    "        Input_set = np.array(list(csv.reader(file, delimiter=',',quoting=csv.QUOTE_NONNUMERIC)))\n",
    "\n",
    "    with open(labelfile, 'r') as labelfile:\n",
    "        labellist = list(csv.reader(labelfile, delimiter=',',quoting=csv.QUOTE_NONNUMERIC))\n",
    "        Output_set = np.array([item for sublist in labellist for item in sublist])\n",
    "    \n",
    "    desired_out = np.zeros([len(Output_set), 8], dtype = int)\n",
    "\n",
    "    for i,v in enumerate(Output_set):\n",
    "        desired_out[int(i), int(v-1),] = 1\n",
    "    \n",
    "    \n",
    "    return Input_set, desired_out, Output_set\n",
    "\n",
    "\n",
    "def activation_prompt(layer):\n",
    "    \"\"\"\n",
    "    Chooses what activation function to use. \n",
    "        Activation Function \n",
    "        0 - Logistic Function\n",
    "        1 - Tangent Function\n",
    "        2 - ReLu\n",
    "    Returns\n",
    "    -------\n",
    "        Activation Function\n",
    "        Slope Parameter or Tanh Parameter a\n",
    "        0 or Tanh Parameter b\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Activation Function:')\n",
    "    print('0: Logistic')\n",
    "    print('1: Tangent Hyperbolic')\n",
    "    print('2: ReLu')\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            question = int(input('Activation Function for '+ layer + ': '))\n",
    "            break\n",
    "        except:\n",
    "            print(\"That's not a valid option!\")\n",
    "\n",
    "    if question == 0:\n",
    "        print('Activation Function: Logistic')\n",
    "        slope = float(input('Logistic Slope Parameter (Default is 2): ') or '2')\n",
    "        return 0, slope, 0\n",
    "    elif question == 1:\n",
    "        \n",
    "        print('Activation Function: Tangent Hyperbolic')\n",
    "        tanh_a = float(input('Tanh a Parameter (Default is 1.716): ') or '1.716')\n",
    "        tanh_b = float(input('Tanh b Parameter (Default is 2/3): ') or str(2/3))\n",
    "        return 1, tanh_a, tanh_b\n",
    "    elif question == 2:\n",
    "        print('Activation Function: Leaky ReLu')\n",
    "        slope1 = float(input('Input > 0 Slope Parameter (Default is 1): ') or '1')\n",
    "        slope2 = float(input('Input < 0 Slope Parameter (Default is 0.01): ') or '0.01')\n",
    "        return 2, slope1, slope2\n",
    "    else:\n",
    "        print('That\\'s not an option!')\n",
    "        return np.nan,np.nan,np.nan\n",
    "    \n",
    "def activation_function (H):\n",
    "    \"\"\"\n",
    "    Calls the activation_prompt to initialize the activation functions per layer.\n",
    "    \"\"\"\n",
    "    af = {}\n",
    "    for i in range(H):\n",
    "        l = str(i+1)\n",
    "        layer = 'Hidden Layer ' + l\n",
    "        p = activation_prompt(layer)\n",
    "        af['af_'+ l]= p\n",
    "    p = activation_prompt('Outer Layer')\n",
    "    af['af_out'] = p\n",
    "    return af\n",
    "    \n",
    "def weights (H,Input,Output):\n",
    "    \"\"\"Determine the Initial Weights and Delta Weights based on the number of nodes\n",
    "        H: Number of Hidden Layer\n",
    "        Input: array of numerics\n",
    "            one instance with 354 features of the Input Set\n",
    "        Output: array of numerics\n",
    "            8 output classes\n",
    "    \"\"\"\n",
    "    n = len(Input) +1\n",
    "    m = len(Output)\n",
    "    w = {}\n",
    "    dw = {}\n",
    "    nodes = int(input('Number of Nodes in 1st Hidden Layer (Default 100): ') or '100')\n",
    "    for i in range(H):\n",
    "        l = str(i+1)\n",
    "        \n",
    "        w['w_h'+l] = np.random.randn(nodes,n)*np.sqrt(2/n)\n",
    "        dw['dw_h'+l] = np.zeros((nodes,n))\n",
    "        n = nodes+1\n",
    "        nodes = n+1\n",
    "    w['w_out'] = np.random.randn(m,n)*np.sqrt(2/n)\n",
    "    dw['dw_out'] = np.zeros((m,n))\n",
    "    \n",
    "    return w,dw\n",
    "\n",
    "\n",
    "def output(p, v):\n",
    "    \"\"\"\n",
    "    Calculates the Output value. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p : tuple\n",
    "        Activation Function:\n",
    "            0 - Logistic Function\n",
    "            1 - Tangent Function\n",
    "            2 - ReLu\n",
    "        a : Numeric\n",
    "            For Logistic, enter the logistic slope parameter.\n",
    "            For Tangent, enter the tanh parameter a\n",
    "        b : Numeric\n",
    "            For Tangent, enter the tanh parameter b.\n",
    " \n",
    "    v : Numeric\n",
    "        Internal activity, summation of the product of the inputs and the weights minus the bias.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Output\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if p[0] == 0: #Logistic\n",
    "        output = 1/(1+np.exp(-v*p[1]))\n",
    "        return output\n",
    "    elif p[0] == 1: #TanH\n",
    "        output = p[1]*np.tanh(p[2]*v) \n",
    "        return output\n",
    "    elif p[0] == 2:#Leaky ReLu\n",
    "        output = np.maximum(p[2]*v,p[1]*v)\n",
    "        return output\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def forward(Input, H, w, af):\n",
    "    \"\"\" \n",
    "    Forward Propagation:\n",
    "        Input: batch of input values\n",
    "        H: Number of Hidden Layers\n",
    "        w: Weights\n",
    "        af: activation function per layer\n",
    "    Returns\n",
    "    -------    \n",
    "        v: itnernal activity of batch\n",
    "        y: output of batch\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    v = {}\n",
    "    y = {}\n",
    "    \n",
    "    #Hidden Layer\n",
    "    for i in range(H):\n",
    "        l = str(i+1) \n",
    "        v['v_h'+l] = np.einsum('ij,kj->ik', Input, w['w_h'+l])\n",
    "        y['y_h'+ l] = output(af['af_'+ l],v['v_h'+ l])\n",
    "        Input = np.insert(y['y_h'+l], 0, 1, axis = 1)\n",
    "    #Output Layer   \n",
    "    v['v_out'] =np.einsum('ij,kj->ik', Input, w['w_out'])\n",
    "    y['y_out'] = output(af['af_out'],v['v_out'])\n",
    "    \n",
    "    return v,y    \n",
    "    \n",
    "def error(d, y):\n",
    "    \"\"\"\n",
    "    Calcualtes the error to be used for backpropagation\n",
    "    d: desired output\n",
    "    y: actual output\n",
    "    --------\n",
    "    e: Error\n",
    "    \"\"\"\n",
    "\n",
    "    e = d-y\n",
    "    return e\n",
    "\n",
    "def reluDerivative(x,a,b):\n",
    "    \"\"\"\n",
    "    Calculates the derivative of ReLu\n",
    "    \"\"\"\n",
    "    x[x<=0] = b\n",
    "    x[x>0] = a\n",
    "    return x\n",
    "\n",
    "def delta_function(layer,af,e,y,w,d):\n",
    "    \"\"\"\n",
    "    Calculates the delta gradient based on the layer and activation function\n",
    "        layer: Outer or Hidden Layer\n",
    "            0: Outer Layer\n",
    "            1: Hidden Layer\n",
    "        af: activation function\n",
    "            0: Logistic\n",
    "            1: TanH\n",
    "            2: Leaky Relu\n",
    "        e: batch error\n",
    "        y: batch actual output\n",
    "        w: weights\n",
    "        d: previous delta\"\"\"   \n",
    "    \n",
    "    if layer == 0: #Outer Layer\n",
    "        if af[0] == 0: #Logistic\n",
    "            delta = af[1]*e*y*(1-y)\n",
    "            \n",
    "        elif af[0] == 1: #Tanh\n",
    "            delta = (af[2]/af[1])*e*(af[1]-y)*(af[1]+y)\n",
    "        \n",
    "        elif af[0] == 2: #Leaky ReLu\n",
    "            delta = np.where(y > 0, y, y * af[1]) \n",
    "            \n",
    "        else:\n",
    "            delta = np.nan\n",
    "    \n",
    "    if layer == 1: #Hidden Layer\n",
    "        if af[0] == 0: #Logistic\n",
    "            delta =  af[1]*y*(1-y)*np.einsum('ij,jk->ik', d,  np.delete(w, (0), axis=1))             \n",
    "         \n",
    "        elif af[0] == 1: #Tanh\n",
    "            delta = (af[2]/af[1])*(af[1]-y)*(af[1]+y)*np.einsum('ij,jk->ik', d,  np.delete(w, (0), axis=1))\n",
    "        \n",
    "        elif af[0] == 2: #Leaky ReLu\n",
    "            delta = reluDerivative(y, af[1],af[2])*np.einsum('ij,jk->ik', d,  np.delete(w, (0), axis=1))\n",
    "        \n",
    "            \n",
    "        else:\n",
    "            delta = np.nan\n",
    "    \n",
    "    return delta\n",
    "\n",
    "\n",
    "def gradient(H,e,af,y,w):\n",
    "    \"\"\"\n",
    "    Calculates the delta gradient based on the activation function and the layer\n",
    "        H: Number of Hidden Layers\n",
    "        e: batch error\n",
    "        af: activation function\n",
    "        y: batch actual output\n",
    "        w: weights\n",
    "    \"\"\"\n",
    "    delta = {}\n",
    "    \n",
    "    #outer\n",
    "    delta['d_out'] = delta_function(0,af['af_out'],e,y['y_out'],w['w_out'],0)\n",
    "    \n",
    "    l = H\n",
    "    w_i = w['w_out']\n",
    "    d_i = delta['d_out']\n",
    "    #hidden layers\n",
    "    for i in range(H):\n",
    "        j = str(l)\n",
    "        delta['d_h'+ j] = delta_function(1,af['af_' + j],e,y['y_h'+ j],w_i,d_i)\n",
    "        l = l-1\n",
    "        w_i = w['w_h'+j]\n",
    "        d_i = delta['d_h'+ j]\n",
    "    \n",
    "    return delta\n",
    "\n",
    "def delta_weights(H,Input,a,delta,y,batchsize):\n",
    "    \"\"\"\n",
    "    Delta weights correction\n",
    "        H: Number of Hidden Layers\n",
    "        Input: batch Input\n",
    "        a: learning rate\n",
    "        delta: delta gradient\n",
    "        y: batch actual output\n",
    "        batchsize: number of Input sets in the batch\n",
    "    \"\"\"\n",
    "    \n",
    "    dw = {}\n",
    "    for i in range(H):\n",
    "        l = str(i+1)\n",
    "        \n",
    "        dw['dw_h'+l] = (a*np.einsum('ij,kl->lj', Input, delta['d_h'+l]))/np.square(len(Input))\n",
    "        Input = np.insert(y['y_h'+l], 0, 1, axis = 1)\n",
    "            \n",
    "    dw['dw_out'] = (a*np.einsum('ij,kl->lj', Input, delta['d_out']))/np.square(len(Input))\n",
    "   \n",
    "    return dw\n",
    "\n",
    "\n",
    "def new_weights (H,m,w1,dw_old,dw):\n",
    "    \"\"\"\n",
    "    Weights update using the Generalized Delta Rule\n",
    "        H: Number of Hidden Layers\n",
    "        m: momentum\n",
    "        w1: weights\n",
    "        dw_old: old delta weights\n",
    "        dw: new delta weights\n",
    "    \"\"\"\n",
    "    \n",
    "    w = {}\n",
    "    for i in range(H):\n",
    "        l = str(i+1)\n",
    "        w['w_h'+l] = w1['w_h'+l] +  dw['dw_h'+l] + m*dw_old['dw_h'+l] \n",
    "    \n",
    "    w['w_out'] = w1['w_out'] + dw['dw_out'] + m*dw_old['dw_out'] \n",
    "        \n",
    "    return w\n",
    "\n",
    "#%%\n",
    "#Start Testing\n",
    "\n",
    "bias = 1\n",
    "\n",
    "print(\"Loading Training and Validation Files. Please Wait.\")\n",
    "#Initialize dataset\n",
    "setfile = 'training_set.csv'\n",
    "labelfile = 'training_label.csv'\n",
    "InputSet,DesiredOutputSet,OutputSet = Get_Files(setfile,labelfile)\n",
    "\n",
    "#Initialize validation dataset\n",
    "setfile = 'validation_set.csv'\n",
    "labelfile = 'validation_label.csv'\n",
    "InputValidationSet,DesiredOutputValidationSet,OutputValidationSet = Get_Files(setfile,labelfile)\n",
    "InputValidationSet = np.insert(InputValidationSet, 0, bias, axis=1)\n",
    "\n",
    "#Shuffle Inputset\n",
    "df = np.column_stack((InputSet,DesiredOutputSet))\n",
    "df_shuffle = np.take(df,np.random.permutation(df.shape[0]),axis=0,out=df) \n",
    "outputsize = len(DesiredOutputSet[0,:])\n",
    "inputsize = len(InputSet[0,:])\n",
    "\n",
    "#Enter User Inputs\n",
    "Network = \"Network\" + str(input('Network: '))\n",
    "H = int(input('Number of Hidden Layers (Default 2): ') or '2')\n",
    "batchsize = int(input('Batch Size (Default 8): ') or '8')\n",
    "a = float(input('Learning Rate (Default 0.1): ') or '0.1')\n",
    "ainit = a\n",
    "m = float(input('Momentum (Default 0.9): ') or '0.9')\n",
    "minit = m\n",
    "af = activation_function(H)\n",
    "w,dw_0 = weights(H,InputSet[0], DesiredOutputSet[0])\n",
    "epoch = int(input('Number of Epoch (Default 5): ') or '5')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Start Training\n",
    "epocherror = []\n",
    "validation_error = []\n",
    "misclassification = []\n",
    "lt=[]\n",
    "mplot=[]\n",
    "print('Start of Learning: ---'+ str(start_time) +'---')\n",
    "for k in range(epoch):\n",
    "    epochstart_time = time.time()\n",
    "    final_output = []\n",
    "    sumerror = []\n",
    "    lt.append(a)\n",
    "    mplot.append(m)\n",
    "    #Shuffle Input Set\n",
    "    DesiredOutputSet = np.delete(df_shuffle, np.s_[:inputsize], axis=1)  \n",
    "    InputSet = np.insert(np.delete(df_shuffle, np.s_[inputsize:], axis=1), 0, bias, axis=1)\n",
    "    InputBatch = [InputSet[i:i + batchsize] for i in range(0, len(InputSet), batchsize)]\n",
    "    DesiredOutputBatch = [DesiredOutputSet[i:i + batchsize] for i in range(0, len(DesiredOutputSet), batchsize)]\n",
    "    \n",
    "    for i in range(len(DesiredOutputBatch)):\n",
    "        \n",
    "        Input = InputBatch[i]\n",
    "        d = DesiredOutputBatch[i]\n",
    "       \n",
    "        v,y = forward(Input,2,w,af)\n",
    "        e = error(d,y['y_out'])\n",
    "        squareerror = 1/(2*len(d[0]))*np.sum(np.square(e))\n",
    "        \n",
    "        delta = gradient(H,e,af,y,w)\n",
    "        dw = delta_weights(H, Input, a, delta, y, batchsize)\n",
    "        w = new_weights(H, m, w, dw_0, dw)\n",
    "        dw_0 = dw\n",
    "        y_output = np.argmax(y['y_out'], axis=0)+1\n",
    "        final_output = np.append(final_output, y_output)\n",
    "        sumerror.append(squareerror)\n",
    "        \n",
    "    epocherror.append(np.average(sumerror))\n",
    "    \n",
    "    if (k+1) %5 == 0: \n",
    "        InputValidationBatch = [InputValidationSet[i:i + batchsize] for i in range(0, len(InputValidationSet), batchsize)]\n",
    "        DesiredValidationOutputBatch = [DesiredOutputValidationSet[i:i + batchsize] for i in range(0, len(DesiredOutputValidationSet), batchsize)]\n",
    "        y_predicted = []\n",
    "        val_e = []\n",
    "        for i in range(len(DesiredValidationOutputBatch)):\n",
    "            Input = InputValidationBatch[i]\n",
    "            d1 = DesiredValidationOutputBatch[i]\n",
    "            outputs = forward(Input,2,w,af)[1]\n",
    "            validation_e = error(d1,outputs['y_out'])\n",
    "            PredictedOutput = np.argmax(outputs['y_out'], axis=0)+1\n",
    "            val_squareerror = 1/(2*batchsize)*np.sum(np.square(validation_e))\n",
    "            \n",
    "            val_e.append(val_squareerror)\n",
    "            y_predicted = np.append(y_predicted,PredictedOutput )\n",
    "        validation_error.append(np.average(val_e))\n",
    "        \n",
    "        classes = len(DesiredOutputSet[0]) #find number of classes\n",
    "\n",
    "        cm = [[sum([((OutputValidationSet)[i] == true_class) and (y_predicted[i] == pred_class) \n",
    "                        for i in range(len(OutputValidationSet))])\n",
    "                   for pred_class in range(1, classes + 1)] \n",
    "                   for true_class in range(1, classes + 1)]\n",
    "        cm = np.array(cm,dtype=float)\n",
    "\n",
    "        Total_C = np.sum(cm,axis=0)\n",
    "        Total_R = np.sum(cm,axis=1)\n",
    "        TP = np.diagonal(cm)\n",
    "        FP = Total_C-TP\n",
    "        TN = [sum(Total_C)-Total_C[i]-Total_R[i]+cm[i][i] for i in range(classes)]\n",
    "        FN = Total_R - TP\n",
    "        Recall = TP/(TP+FN)\n",
    "        Precision = TP/(TP+FP)\n",
    "        Accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "        F1Scores = 2*Precision*Recall/(Precision+Recall)\n",
    "        MCC = (TP*TN-FP*FN)/np.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "        misclassification = np.append(misclassification,1-Accuracy)\n",
    "        \n",
    "   # if (k+1) %10 == 0:\n",
    "        a = ainit*1/(1.001+k)\n",
    "        m = minit*(1-k/epoch)/((1-minit)+minit*(1-k/epoch))\n",
    "        \n",
    "    print(\"Per Epoch:--- %s seconds ---\" % (time.time() - epochstart_time))\n",
    "print(\"Total Training Time:--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "plt.plot(np.arange(len(epocherror)),epocherror)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title('Training Error')\n",
    "plt.show()\n",
    "plt.plot(np.arange(0,epoch,5),validation_error)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title('Validation Error')\n",
    "plt.show()\n",
    "plt.plot(np.arange(len(epocherror)),lt)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title('Learning Rate Per Epoch')\n",
    "plt.show()\n",
    "plt.plot(np.arange(len(epocherror)),mplot)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Momentum\")\n",
    "plt.title('Momentum Per Epoch')\n",
    "plt.show()\n",
    "\n",
    "np.savetxt(Network + \"_training_sumofsquares.csv\", np.stack([np.arange(len(epocherror)),epocherror],axis=1), delimiter=\",\")\n",
    "np.savetxt(Network + \"_validation_sumofsquares.csv\", np.stack([np.arange(0,epoch,5),validation_error],axis=1), delimiter=\",\")\n",
    "np.savetxt(Network + \"_validation_misclassification.csv\", misclassification, delimiter=\",\")\n",
    "np.savetxt(Network + \"_confusion_matrix.csv\", cm, delimiter=\",\")\n",
    "np.savetxt(Network + \"_Recall.csv\", Recall, delimiter=\",\")\n",
    "np.savetxt(Network + \"_Precision.csv\", Precision, delimiter=\",\")\n",
    "np.savetxt(Network + \"_Accuracy.csv\", Accuracy, delimiter=\",\")\n",
    "np.savetxt(Network + \"_F1_Scores.csv\", F1Scores, delimiter=\",\")\n",
    "np.savetxt(Network + \"_Matthews_Correlation_Coeff.csv\", MCC, delimiter=\",\")\n",
    "\n",
    "with open(Network + \"_trained_weights.csv\", 'w') as csv_file:  \n",
    "    \n",
    "    for i in range(H):\n",
    "        l = str(i+1)\n",
    "        np.savetxt(csv_file, w['w_h'+l],delimiter=',',header='w_h'+l,fmt='%s')\n",
    "    np.savetxt(csv_file, w['w_out'],delimiter=',',header='w_out',fmt='%s')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe285f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Dataset Predictions\n",
    "\n",
    "H=2\n",
    "bias=1\n",
    "NetworkA = \"Network\" + str(input('Network: '))\n",
    "af_a = activation_function(H)\n",
    "NetworkB = \"Network\" + str(input('Network: '))\n",
    "af_b = activation_function(H)\n",
    "with open(\"NetworkA_trained_weights.csv\", 'r') as file:\n",
    "    NetworkA_w={}\n",
    "    my_reader = csv.reader(file, delimiter=',')\n",
    "\n",
    "    for row in my_reader:\n",
    "        if len(row)==1:\n",
    "           l = row[0].split()[1]\n",
    "           NetworkA_w[l]=[]\n",
    "        else:\n",
    "            row = (list(map(float, row)))\n",
    "            NetworkA_w[l].append(row)\n",
    "\n",
    "\n",
    "with open(\"NetworkB_trained_weights.csv\", 'r') as file:\n",
    "    NetworkB_w={}\n",
    "    my_reader = csv.reader(file, delimiter=',')\n",
    "\n",
    "    for row in my_reader:\n",
    "        if len(row)==1:\n",
    "           l = row[0].split()[1]\n",
    "           NetworkB_w[l]=[]\n",
    "        else:\n",
    "            row = (list(map(float, row)))\n",
    "            NetworkB_w[l].append(row)\n",
    "\n",
    "for i in range(H):\n",
    "    l = str(i+1) \n",
    "    NetworkA_w['w_h'+l] = np.array(NetworkA_w['w_h'+l])\n",
    "    NetworkB_w['w_h'+l] = np.array(NetworkB_w['w_h'+l])\n",
    "#Output Layer   \n",
    "NetworkA_w['w_out'] = np.array(NetworkA_w['w_out'])\n",
    "NetworkB_w['w_out'] = np.array(NetworkB_w['w_out'])\n",
    "\n",
    "filedata = 'C:/Users/KEC/OneDrive - University of the Philippines/MSEE/CS280/PA2/CS280_PA2_Data/CS280_PA2_Data/test_set.csv'\n",
    "\n",
    "with open(filedata, 'r') as file:\n",
    "    testset = np.array(list(csv.reader(file, delimiter=',',quoting=csv.QUOTE_NONNUMERIC)))\n",
    "\n",
    "testset = np.insert(testset, 0, bias, axis=1)\n",
    "\n",
    "testbatch = [testset[i:i + batchsize] for i in range(0, len(testset), batchsize)]\n",
    "\n",
    "NetworkA_Predicted = []\n",
    "NetworkB_Predicted = []\n",
    "for i in range(len(testbatch)):\n",
    "     Input1 = testbatch[i]\n",
    "     \n",
    "     NetworkA_output = forward(Input1,2,NetworkA_w,af_a)[1]\n",
    "     PredictedA = np.argmax(NetworkA_output['y_out'], axis=0)+1\n",
    "     NetworkA_Predicted = np.append(NetworkA_Predicted,PredictedA )    \n",
    "    \n",
    "     NetworkB_output = forward(Input1,2,NetworkB_w,af_b)[1]\n",
    "     PredictedB = np.argmax(NetworkB_output['y_out'], axis=0)+1\n",
    "     NetworkB_Predicted = np.append(NetworkB_Predicted,PredictedB )   \n",
    "\n",
    "np.savetxt(\"predictions_for_test_tanh.csv\", NetworkA_Predicted, delimiter=\",\")\n",
    "np.savetxt(\"predictions_for_test_leakyrelu.csv\", NetworkB_Predicted, delimiter=\",\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
